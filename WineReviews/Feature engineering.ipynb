{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4aafa01",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7402eeb",
   "metadata": {},
   "source": [
    "We will use price, points, winery, title and description features for creating datasets for ML models.\n",
    "\n",
    "Few datasets will be created:\n",
    "   - dataset_desc - contains just description and points features\n",
    "   - dataset_title - contains description, title and points features\n",
    "   - dataset_price - contains description, title, price and points features\n",
    "   - dataset_winery - contains description, title, price, winery and points features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdb1b22",
   "metadata": {},
   "source": [
    "## Description feature encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707deabe",
   "metadata": {},
   "source": [
    "In order to perform machine learning on text documents, we first need to turn the text content into numerical feature vectors.\n",
    "\n",
    "Two approaches will be used:\n",
    "  - **Bag of words** -  process of turning a collection of text documents into numerical feature vectors. This specific strategy (tokenization, counting and normalization) is called the Bag of Words or “Bag of n-grams” representation. Documents are described by word occurrences while completely ignoring the relative position information of the words in the document.\n",
    "  - **Word embeddings** - give us a way to use an efficient, dense representation in which similar words have a similar encoding. An embedding is a dense vector of floating point values. A higher dimensional embedding can capture fine-grained relationships between words, but takes more data to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51062af5",
   "metadata": {},
   "source": [
    "### Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cca5da87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(129971, 13)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "df = pd.read_csv(\"data/winemag-data-130k-v2.csv\", index_col=0)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a99697d",
   "metadata": {},
   "source": [
    "Text preprocessing, tokenizing and filtering of stopwords are all included in CountVectorizer, which builds a dictionary of features and transforms documents to feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1310ab70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(129971, 1000)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer(max_features=1000, stop_words='english')\n",
    "df_count = count_vect.fit_transform(df.description)\n",
    "df_count.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe8a6f8",
   "metadata": {},
   "source": [
    "Occurrence count is a good start but there is an issue: longer documents will have higher average count values than shorter documents, even though they might talk about the same topics.\n",
    "\n",
    "To avoid these potential discrepancies it suffices to divide the number of occurrences of each word in a document by the total number of words in the document: these new features are called tf for Term Frequencies.\n",
    "\n",
    "Another refinement on top of tf is to downscale weights for words that occur in many documents in the corpus and are therefore less informative than those that occur only in a smaller portion of the corpus.\n",
    "\n",
    "This downscaling is called tf–idf for “Term Frequency times Inverse Document Frequency”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6949353c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(129971, 1000)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_transformer = TfidfTransformer(use_idf=True).fit(df_count)\n",
    "df_encode = tf_transformer.transform(df_count)\n",
    "df_encode.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8411b5b",
   "metadata": {},
   "source": [
    "New dataset contains 1000 encoded features which will be fed to ML models.\n",
    "Store first dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ac4a1102",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encode = pd.DataFrame(df_encode.todense())\n",
    "df_encode.to_csv('data/dataset_desc_bag.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0676af7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bebe4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcaebeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b878f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
