{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SequenceTextModelWithPretrainedEmbeddings.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNuONNvL7PvcblYBKBDZhHF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mateopolancecofficial/NLP/blob/main/WineReviews/SequenceTextModelWithPretrainedEmbeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qqLyv17_rWZ",
        "outputId": "a5abcc23-62fd-4289-dff6-f50721fbf790"
      },
      "source": [
        "pip install -q -U tensorflow-text"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 3.4MB 7.5MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGyCqRXl_znV",
        "outputId": "5a0ed4c4-763b-4161-d431-b66f45adcdaf"
      },
      "source": [
        "pip install -q -U tf-models-official"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.1MB 7.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 8.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 358kB 22.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 645kB 27.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 37.6MB 85kB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 54.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 174kB 50.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 15.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 706kB 51.7MB/s \n",
            "\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1KlGeZ6_1f8",
        "outputId": "e4b84a7f-2993-479f-f9c7-7795fa311324"
      },
      "source": [
        "pip install -q -U keras-tuner"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |█████▏                          | 10kB 27.3MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 20kB 18.3MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 30kB 15.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 40kB 13.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 51kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 61kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 5.7MB/s \n",
            "\u001b[?25h  Building wheel for keras-tuner (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VufuyV__3MN",
        "outputId": "94273b6c-9430-4708-c1dd-1ab8b9cef69b"
      },
      "source": [
        "!git clone -l -s https://github.com/mateopolancecofficial/NLP.git"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'NLP'...\n",
            "warning: --local is ignored\n",
            "remote: Enumerating objects: 68, done.\u001b[K\n",
            "remote: Counting objects: 100% (68/68), done.\u001b[K\n",
            "remote: Compressing objects: 100% (60/60), done.\u001b[K\n",
            "remote: Total 68 (delta 31), reused 25 (delta 6), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (68/68), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Es4NC3Qd_45U"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as text  # A dependency of the preprocessing model\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import (\n",
        "    Dense,\n",
        "    Dropout,\n",
        ")\n",
        "import kerastuner as kt\n",
        "from official.nlp import optimization\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "os.environ[\"TFHUB_MODEL_LOAD_FORMAT\"]=\"UNCOMPRESSED\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANV-9gvm_9Ik",
        "outputId": "8a45d20e-ae02-492e-ea07-d778432a3cf7"
      },
      "source": [
        "import os\n",
        "\n",
        "if tf.test.is_gpu_available():\n",
        "  strategy = tf.distribute.MirroredStrategy()\n",
        "  print('Using GPU')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "romsx_M7AD1k"
      },
      "source": [
        "# set parameters\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "path_v1 = \"/content/NLP/WineReviews/data/winemag-data-130k-v2.csv\" \n",
        "path_v2 = \"/content/NLP/WineReviews/data/winemag-data_first150k.csv\"\n",
        "checkpoint_path = \"/content/NLP/WineReviews/checkpoints/cp-{epoch:04d}.ckpt\"\n",
        "batch_size = 1024\n",
        "seed = 42\n",
        "col_idx = 0\n",
        "train_size, test_size, val_size = 0.9, 0.1, 0.1\n",
        "columns = ['description', 'points']"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oaO8T0-AUwd"
      },
      "source": [
        "def load_data(path_v1: str, path_v2: str, columns: list, col_idx: int):\n",
        "  \"\"\"\n",
        "  Load and concatenate two datasets with removing duplicates.\n",
        "  param path_v1: import path of first dataset\n",
        "  param path_v2: import path of second dataset\n",
        "  param columns: list of columns to preserve in dataframe\n",
        "  param col_idx: index of column given in input columns list \n",
        "                 on which look for duplicates in dataframe\n",
        "  return:        pandas dataframe\n",
        "  \"\"\"\n",
        "  \n",
        "  df_v1 = pd.read_csv(path_v1, index_col=0)\n",
        "  df_v1 = df_v1[columns]\n",
        "  # remove numbers form column description from first dataframe\n",
        "  df_v1.description = df_v1.description.str.replace('\\d+', '')\n",
        "\n",
        "  df_v2 = pd.read_csv(path_v2, index_col=0)\n",
        "  df_v2 = df_v2[columns]\n",
        "  # remove numbers form column description from second dataframe\n",
        "  df_v2.description = df_v2.description.str.replace('\\d+', '')\n",
        "\n",
        "  df = pd.concat([df_v1, df_v2])\n",
        "\n",
        "  # dropping duplicte values\n",
        "  df.drop_duplicates(subset = columns[col_idx],\n",
        "                       keep = 'first', inplace = True)\n",
        "  \n",
        "  return df"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcP1H2i-AWsz"
      },
      "source": [
        "def split_data(df: pd.DataFrame, train_size: float, test_size: float, val_size: float):\n",
        "  \"\"\"\n",
        "  Split dataset on train, test and validation subsets.\n",
        "  param df:          input dataframe\n",
        "  param train_size:  fraction of train size\n",
        "  param test_size:   fraction of test size\n",
        "  param val_size:    fraction of validation size\n",
        "  return:            dictionary, keys=names of dataframes, columns=dataframes\n",
        "  \"\"\"\n",
        "  \n",
        "  # shuffle dataset\n",
        "  df = df.sample(frac = 1)\n",
        "  \n",
        "  # split on test and train set\n",
        "  text_train, text_test, y_train, y_test = train_test_split(df.description, df.points,\n",
        "                                           test_size=test_size, train_size=train_size)\n",
        "  \n",
        "  y_train, y_test = y_train.astype('float'), y_test.astype('float')\n",
        "  \n",
        "  # split train set on train and validation subsets\n",
        "  text_train, text_val, y_train, y_val = train_test_split(text_train, y_train,\n",
        "                                                  test_size=val_size, train_size=train_size)\n",
        "  \n",
        "  y_train, y_val = y_train.astype('float'), y_val.astype('float')\n",
        "  \n",
        "\n",
        "  dataset_dict = {\n",
        "      'text_train': text_train,\n",
        "      'y_train': y_train,\n",
        "      'text_val': text_val,\n",
        "      'y_val': y_val,\n",
        "      'text_test': text_test,\n",
        "      'y_test': y_test\n",
        "  }\n",
        "\n",
        "  return dataset_dict"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08SxSFqFAYor"
      },
      "source": [
        "def create_input_datasets(df_data: dict):\n",
        "  \"\"\"\n",
        "  Create tensorflow datasets based on input dataframes for train, validation \n",
        "  and test subsets.\n",
        "  param df_data: dictionary, keys=names of dataframes, columns=dataframes\n",
        "  return:        dictionary, keys=names of datasets, columns=datasets\n",
        "  \"\"\"\n",
        "\n",
        "  # create train dataset for input in tensorflow model\n",
        "  train_dataset = tf.data.Dataset.from_tensor_slices((df_data['text_train'], \n",
        "                                                      df_data['y_train']))\n",
        "  train_dataset = train_dataset.batch(batch_size)\n",
        "  train_ds = train_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "  # create validation dataset for input in tensorflow model\n",
        "  val_dataset = tf.data.Dataset.from_tensor_slices((df_data['text_val'], \n",
        "                                                    df_data['y_val']))\n",
        "  val_dataset = val_dataset.batch(batch_size)\n",
        "  val_ds = val_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "  # create validation dataset for input in tensorflow model\n",
        "  test_dataset = tf.data.Dataset.from_tensor_slices((df_data['text_test'], \n",
        "                                                     df_data['y_test']))\n",
        "  test_dataset = test_dataset.batch(batch_size)\n",
        "  test_ds = test_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "  datasets = {\n",
        "      'train_dataset': train_dataset,\n",
        "      'val_dataset': val_dataset,\n",
        "      'test_dataset': test_dataset\n",
        "  } \n",
        "\n",
        "  return datasets"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeSYjKA3Aabq"
      },
      "source": [
        "# define loss functions\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "def rmse():\n",
        "  def root_mean_squared_error(y_true, y_pred):\n",
        "        return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
        "  return root_mean_squared_error\n",
        "\n",
        "def rmsle():\n",
        "  def root_mean_squared_log_error(y_true, y_pred):\n",
        "    return K.sqrt(K.mean(K.square(K.log(1+y_pred) - K.log(1+y_true))))\n",
        "  return root_mean_squared_log_error"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4zPCCXFAcID"
      },
      "source": [
        "# disable eager execution\n",
        "tf.config.run_functions_eagerly(False)\n",
        "\n",
        "def model_builder(hp):\n",
        "  # use pretrained embeddings for input layer\n",
        "  hub_model = \"https://tfhub.dev/google/nnlm-en-dim128-with-normalization/2\"\n",
        "  # 'trainable=True' - boolean controlling whether this layer is trainable\n",
        "  hub_layer = hub.KerasLayer(hub_model, input_shape=[], dtype=tf.string, \n",
        "                             trainable=True)\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(hub_layer)\n",
        "  hp_units_1 = hp.Int('units_1', min_value=64, max_value=128, step=16)\n",
        "  activation=hp.Choice(\n",
        "        'dense_activation',\n",
        "        values=['relu', 'tanh', 'sigmoid'],\n",
        "        default='relu'\n",
        "    )\n",
        "  model.add(Dense(units=hp_units_1, activation=activation))\n",
        "  model.add(\n",
        "            Dropout(rate=hp.Float(\n",
        "                'dropout_1',\n",
        "                min_value=0.1,\n",
        "                max_value=0.5,\n",
        "                default=0.25,\n",
        "                step=0.1,\n",
        "            ))\n",
        "        )\n",
        "  hp_units_2 = hp.Int('units_2', min_value=8, max_value=64, step=16)\n",
        "  model.add(Dense(units=hp_units_2, activation=activation))\n",
        "  model.add(\n",
        "            Dropout(rate=hp.Float(\n",
        "                'dropout_2',\n",
        "                min_value=0.1,\n",
        "                max_value=0.5,\n",
        "                default=0.25,\n",
        "                step=0.1,\n",
        "            ))\n",
        "        )\n",
        "  model.add(Dense(1))\n",
        "\n",
        "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "  \n",
        "  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "              loss=rmse(),\n",
        "              metrics=['mean_absolute_error'])\n",
        "\n",
        "  return model\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgstlXwRAeVz"
      },
      "source": [
        "# call data transformation functions\n",
        "df = load_data(path_v1, path_v2, columns, col_idx)\n",
        "df_data = split_data(df, train_size, test_size, val_size)\n",
        "datasets = create_input_datasets(df_data)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HijT1DhkAihz"
      },
      "source": [
        "tuner = kt.Hyperband(model_builder,\n",
        "                     objective='mean_absolute_error',\n",
        "                     max_epochs=5,\n",
        "                     directory='SequenceTextPretrained'\n",
        "                    )\n",
        "                    "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "gux06kD6AkgT",
        "outputId": "8500694c-0dd1-4a42-e590-9799ee1ca760"
      },
      "source": [
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "tuner.search(df_data['text_train'], df_data['y_train'], \n",
        "             validation_data=(df_data['text_val'], df_data['y_val']), \n",
        "             epochs=5, callbacks=[stop_early])\n",
        "\n",
        "# Get the optimal hyperparameters\n",
        "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 10 Complete [00h 25m 43s]\n",
            "mean_absolute_error: 5.760629653930664\n",
            "\n",
            "Best mean_absolute_error So Far: 2.22566556930542\n",
            "Total elapsed time: 02h 40m 27s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-6cc49bf58cb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Get the optimal hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mbest_hps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mbest_hps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'units'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/kerastuner/engine/hyperparameters.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{} is currently inactive.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{} does not exist.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: units does not exist."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6v1qG3-uJMJA"
      },
      "source": [
        "model = tuner.hypermodel.build(best_hps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUAnA6yEAoFM"
      },
      "source": [
        "history = model.fit(x=train_ds,\n",
        "                    validation_data=val_ds,\n",
        "                    epochs=50,\n",
        "                    callbacks=[es_callback]\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}