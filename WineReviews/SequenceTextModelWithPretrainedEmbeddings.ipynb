{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SequenceTextModelWithPretrainedEmbeddings.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPc624GzG5I9RH3iOwBas+m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mateopolancecofficial/NLP/blob/main/WineReviews/SequenceTextModelWithPretrainedEmbeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqTdDhjhaky3"
      },
      "source": [
        "### Sequence Text Model with pretrained embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yF6kO-4a2mE",
        "outputId": "6d11c139-d5f1-4c25-ed6e-cb18d4aa3bed"
      },
      "source": [
        "pip install -q -U tensorflow-text"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 3.4MB 5.3MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lhLaGjJa6s4",
        "outputId": "c95ff3fe-4a8c-4311-a0cb-acf1b993ba8a"
      },
      "source": [
        "pip install -q -U tf-models-official"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.1MB 6.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 16.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 706kB 50.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 9.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 174kB 32.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 37.6MB 3.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 358kB 46.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 645kB 37.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 5.6MB/s \n",
            "\u001b[?25h  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGUL-TElz3cz",
        "outputId": "7588ead2-5189-4bea-a320-1da8760d4e14"
      },
      "source": [
        "pip install -q -U keras-tuner"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |█████▏                          | 10kB 18.3MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 20kB 12.8MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 30kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 40kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 51kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 61kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 3.1MB/s \n",
            "\u001b[?25h  Building wheel for keras-tuner (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bm0JQeBtbANu",
        "outputId": "ddfd0b0c-00fb-4bc6-db71-db1a2f025242"
      },
      "source": [
        "!git clone -l -s https://github.com/mateopolancecofficial/NLP.git"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'NLP'...\n",
            "warning: --local is ignored\n",
            "remote: Enumerating objects: 60, done.\u001b[K\n",
            "remote: Counting objects: 100% (60/60), done.\u001b[K\n",
            "remote: Compressing objects: 100% (52/52), done.\u001b[K\n",
            "remote: Total 60 (delta 26), reused 26 (delta 6), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (60/60), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8TfWf5taqot"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as text  # A dependency of the preprocessing model\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow import keras\n",
        "import kerastuner as kt\n",
        "from official.nlp import optimization\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "os.environ[\"TFHUB_MODEL_LOAD_FORMAT\"]=\"UNCOMPRESSED\""
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AJJt0MmbSvV",
        "outputId": "9dda86bf-50be-442b-da1b-c5526ee25df8"
      },
      "source": [
        "import os\n",
        "\n",
        "if os.environ['COLAB_TPU_ADDR']:\n",
        "  cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
        "  tf.config.experimental_connect_to_cluster(cluster_resolver)\n",
        "  tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
        "  strategy = tf.distribute.TPUStrategy(cluster_resolver)\n",
        "  print('Using TPU')\n",
        "elif tf.test.is_gpu_available():\n",
        "  strategy = tf.distribute.MirroredStrategy()\n",
        "  print('Using GPU')\n",
        "else:\n",
        "  raise ValueError('Running on CPU is not recommended.')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Py7UzAj7nHP6"
      },
      "source": [
        "# set parameters\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "path_v1 = \"/content/NLP/WineReviews/data/winemag-data-130k-v2.csv\" \n",
        "path_v2 = \"/content/NLP/WineReviews/data/winemag-data_first150k.csv\"\n",
        "checkpoint_path = \"/content/NLP/WineReviews/checkpoints/cp-{epoch:04d}.ckpt\"\n",
        "batch_size = 2048\n",
        "seed = 42\n",
        "col_idx = 0\n",
        "train_size, test_size, val_size = 0.9, 0.1, 0.1\n",
        "columns = ['description', 'points']"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PL_jaVambdL-"
      },
      "source": [
        "def load_data(path_v1: str, path_v2: str, columns: list, col_idx: int):\n",
        "  \"\"\"\n",
        "  Load and concatenate two datasets with removing duplicates.\n",
        "  param path_v1: import path of first dataset\n",
        "  param path_v2: import path of second dataset\n",
        "  param columns: list of columns to preserve in dataframe\n",
        "  param col_idx: index of column given in input columns list \n",
        "                 on which look for duplicates in dataframe\n",
        "  return:        pandas dataframe\n",
        "  \"\"\"\n",
        "  \n",
        "  df_v1 = pd.read_csv(path_v1, index_col=0)\n",
        "  df_v1 = df_v1[columns]\n",
        "  # remove numbers form column description from first dataframe\n",
        "  df_v1.description = df_v1.description.str.replace('\\d+', '')\n",
        "\n",
        "  df_v2 = pd.read_csv(path_v2, index_col=0)\n",
        "  df_v2 = df_v2[columns]\n",
        "  # remove numbers form column description from second dataframe\n",
        "  df_v2.description = df_v2.description.str.replace('\\d+', '')\n",
        "\n",
        "  df = pd.concat([df_v1, df_v2])\n",
        "\n",
        "  # dropping duplicte values\n",
        "  df.drop_duplicates(subset = columns[col_idx],\n",
        "                       keep = 'first', inplace = True)\n",
        "  \n",
        "  return df"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2YuOOGRfNF9"
      },
      "source": [
        "def split_data(df: pd.DataFrame, train_size: float, test_size: float, val_size: float):\n",
        "  \"\"\"\n",
        "  Split dataset on train, test and validation subsets.\n",
        "  param df:          input dataframe\n",
        "  param train_size:  fraction of train size\n",
        "  param test_size:   fraction of test size\n",
        "  param val_size:    fraction of validation size\n",
        "  return:            dictionary, keys=names of dataframes, columns=dataframes\n",
        "  \"\"\"\n",
        "  \n",
        "  # shuffle dataset\n",
        "  df = df.sample(frac = 1)\n",
        "  \n",
        "  # split on test and train set\n",
        "  text_train, text_test, y_train, y_test = train_test_split(df.description, df.points,\n",
        "                                           test_size=test_size, train_size=train_size)\n",
        "  \n",
        "  y_train, y_test = y_train.astype('float'), y_test.astype('float')\n",
        "  \n",
        "  # split train set on train and validation subsets\n",
        "  text_train, text_val, y_train, y_val = train_test_split(text_train, y_train,\n",
        "                                                  test_size=val_size, train_size=train_size)\n",
        "  \n",
        "  y_train, y_val = y_train.astype('float'), y_val.astype('float')\n",
        "  \n",
        "\n",
        "  dataset_dict = {\n",
        "      'text_train': text_train,\n",
        "      'y_train': y_train,\n",
        "      'text_val': text_val,\n",
        "      'y_val': y_val,\n",
        "      'text_test': text_test,\n",
        "      'y_test': y_test\n",
        "  }\n",
        "\n",
        "  return dataset_dict"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vyF-aqJj9Um"
      },
      "source": [
        "def create_input_datasets(df_data: dict):\n",
        "  \"\"\"\n",
        "  Create tensorflow datasets based on input dataframes for train, validation \n",
        "  and test subsets.\n",
        "  param df_data: dictionary, keys=names of dataframes, columns=dataframes\n",
        "  return:        dictionary, keys=names of datasets, columns=datasets\n",
        "  \"\"\"\n",
        "\n",
        "  # create train dataset for input in tensorflow model\n",
        "  train_dataset = tf.data.Dataset.from_tensor_slices((df_data['text_train'], \n",
        "                                                      df_data['y_train']))\n",
        "  train_dataset = train_dataset.batch(batch_size)\n",
        "  train_ds = train_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "  # create validation dataset for input in tensorflow model\n",
        "  val_dataset = tf.data.Dataset.from_tensor_slices((df_data['text_val'], \n",
        "                                                    df_data['y_val']))\n",
        "  val_dataset = val_dataset.batch(batch_size)\n",
        "  val_ds = val_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "  # create validation dataset for input in tensorflow model\n",
        "  test_dataset = tf.data.Dataset.from_tensor_slices((df_data['text_test'], \n",
        "                                                     df_data['y_test']))\n",
        "  test_dataset = test_dataset.batch(batch_size)\n",
        "  test_ds = test_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "  datasets = {\n",
        "      'train_dataset': train_dataset,\n",
        "      'val_dataset': val_dataset,\n",
        "      'test_dataset': test_dataset\n",
        "  } \n",
        "\n",
        "  return datasets"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4eJsL_Tj9hl"
      },
      "source": [
        "# define loss functions\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "def rmse():\n",
        "  def root_mean_squared_error(y_true, y_pred):\n",
        "        return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
        "  return root_mean_squared_error\n",
        "\n",
        "def rmsle():\n",
        "  def root_mean_squared_log_error(y_true, y_pred):\n",
        "    return K.sqrt(K.mean(K.square(K.log(1+y_pred) - K.log(1+y_true))))\n",
        "  return root_mean_squared_log_error"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNlzb6Dqj9ut"
      },
      "source": [
        "# disable eager execution\n",
        "tf.config.run_functions_eagerly(False)\n",
        "\n",
        "def model_builder(hp):\n",
        "  # use pretrained embeddings for input layer\n",
        "  hub_model = \"https://tfhub.dev/google/nnlm-en-dim128-with-normalization/2\"\n",
        "  # 'trainable=True' - boolean controlling whether this layer is trainable\n",
        "  hub_layer = hub.KerasLayer(hub_model, input_shape=[], dtype=tf.string, \n",
        "                             trainable=True)\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(hub_layer)\n",
        "  hp_units = hp.Int('units', min_value=64, max_value=128, step=16)\n",
        "  model.add(keras.layers.Dense(units=hp_units, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "  hp_units = hp.Int('units', min_value=8, max_value=64, step=16)\n",
        "  model.add(keras.layers.Dense(units=hp_units, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "  model.add(tf.keras.layers.Dense(1))\n",
        "  \n",
        "  model.compile(optimizer='rmsprop',\n",
        "              loss=rmse(),\n",
        "              metrics=['mean_absolute_error'])\n",
        "\n",
        "  return model\n"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soActJibf_MB"
      },
      "source": [
        "# call data transformation functions\n",
        "df = load_data(path_v1, path_v2, columns, col_idx)\n",
        "df_data = split_data(df, train_size, test_size, val_size)\n",
        "datasets = create_input_datasets(df_data)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PCg0N2PgBA5"
      },
      "source": [
        "tuner = kt.Hyperband(model_builder,\n",
        "                     objective='mean_absolute_error',\n",
        "                     max_epochs=10,\n",
        "                    )\n",
        "                    "
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "id": "aN6QXu61l88k",
        "outputId": "5be515a0-3e50-4ac9-eb79-bea1eacc0bd3"
      },
      "source": [
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "tuner.search(datasets['train_dataset'], datasets['val_dataset'], \n",
        "             epochs=50, callbacks=[stop_early])\n",
        "\n",
        "# Get the optimal hyperparameters\n",
        "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "print(f\"\"\"\n",
        "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
        "layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
        "is {best_hps.get('learning_rate')}.\n",
        "\"\"\")"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Search: Running Trial #2\n",
            "\n",
            "Hyperparameter    |Value             |Best Value So Far \n",
            "units             |112               |?                 \n",
            "tuner/epochs      |2                 |?                 \n",
            "tuner/initial_e...|0                 |?                 \n",
            "tuner/bracket     |2                 |?                 \n",
            "tuner/round       |0                 |?                 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-79-352479ff8f7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m tuner.search(datasets['train_dataset'], datasets['val_dataset'], \n\u001b[0;32m----> 4\u001b[0;31m              epochs=50, callbacks=[stop_early])\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Get the optimal hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/kerastuner/engine/base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_search_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/kerastuner/tuners/hyperband.py\u001b[0m in \u001b[0;36mrun_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mfit_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tuner/epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mfit_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'initial_epoch'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tuner/initial_epoch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHyperband\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_build_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/kerastuner/engine/multi_execution_tuner.py\u001b[0m in \u001b[0;36mrun_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mcopied_fit_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'callbacks'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_and_fit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopied_fit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_values\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirection\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'min'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/kerastuner/engine/tuner.py\u001b[0m in \u001b[0;36m_build_and_fit_model\u001b[0;34m(self, trial, fit_args, fit_kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \"\"\"\n\u001b[1;32m    140\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1062\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, steps, **kwargs)\u001b[0m\n\u001b[1;32m    691\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_user_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_validate_args\u001b[0;34m(self, y, sample_weights, steps)\u001b[0m\n\u001b[1;32m    724\u001b[0m     \u001b[0;31m# Arguments that shouldn't be passed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_none_or_empty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m       raise ValueError(\"`y` argument is not supported when using \"\n\u001b[0m\u001b[1;32m    727\u001b[0m                        \"dataset as input.\")\n\u001b[1;32m    728\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_none_or_empty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: `y` argument is not supported when using dataset as input."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xY4H0MOa2yaj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}